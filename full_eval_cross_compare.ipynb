{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8825f36-5b92-4975-ab36-cf00c6042e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyphenate import hyphenate_word\n",
    "from model import sp_syllabler\n",
    "import pickle\n",
    "from nltk.metrics.distance import edit_distance\n",
    "import pandas as pd\n",
    "from hyphenate import hyphenate_word\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import syllables\n",
    "import pyphen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42146bbf-c8fd-421c-bef5-83d6d1c6ea93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "# prepping and calling functions\n",
    "\n",
    "def get_probability(y_val):\n",
    "    num_pos = 0\n",
    "    num_neg = 0\n",
    "    total = 0\n",
    "    for word in y_val:\n",
    "        for c in word:\n",
    "            if c == 2:\n",
    "                num_pos += 1\n",
    "                total += 1\n",
    "            elif c == 1:\n",
    "                num_neg += 1\n",
    "                total += 1\n",
    "    return float(num_pos)/float(total)\n",
    "\n",
    "def calc_brier(attempted, probability):\n",
    "    total = 0\n",
    "    sum_brier = 0\n",
    "    for word in attempted:\n",
    "        for c in attempted:\n",
    "            total += 1\n",
    "            if c == 2:\n",
    "                sum_brier += (probability - 1)**2\n",
    "            elif c == 1:\n",
    "                sum_brier += (probability - 0)**2\n",
    "    return (1./total)*(sum_brier)\n",
    "\n",
    "def calc_f1(attempted, true):\n",
    "    true_pos = 0\n",
    "    true_neg = 0\n",
    "    false_pos = 0\n",
    "    false_neg = 0\n",
    "    correct_num_char = 0\n",
    "    total_checked = 0\n",
    "    for i in range(0, len(attempted)):\n",
    "        total_checked += 1\n",
    "        if (len(attempted[i]) == len(true[i])):\n",
    "            correct_num_char += 1\n",
    "            for j in range(0, len(attempted[i])):\n",
    "                if(attempted[i][j] == true[i][j]):\n",
    "                    if true[i][j] == 1:\n",
    "                        true_neg += 1\n",
    "                    elif true[i][j] == 2:\n",
    "                        true_pos += 1\n",
    "                else:\n",
    "                    if true[i][j] == 1:\n",
    "                        false_pos += 1\n",
    "                    elif true[i][j] == 2:\n",
    "                        false_neg += 1\n",
    "    \n",
    "    precision = true_pos/(true_pos+false_pos)\n",
    "    recall = true_pos/(true_pos + false_neg)\n",
    "    f_one = 2/((1/precision)+(1/recall))\n",
    "    \n",
    "    return total_checked, correct_num_char, true_pos, true_neg, false_pos, false_neg, precision, recall, f_one\n",
    "\n",
    "def convert_to_hot(syl_word):\n",
    "    hot = []\n",
    "    i = 0\n",
    "    while  i < len(syl_word):\n",
    "        if i == len(syl_word) - 1:\n",
    "            hot += [1]\n",
    "            return hot\n",
    "        if syl_word[i+1] == '-':\n",
    "            hot += [2]\n",
    "            i += 2\n",
    "        else:\n",
    "            hot += [1]\n",
    "            i += 1\n",
    "    return hot\n",
    "\n",
    "def to_categorical(sequences):\n",
    "        cat_sequences = []\n",
    "        for s in sequences:\n",
    "            cats = []\n",
    "            for item in s:\n",
    "                cats.append(np.zeros(3))\n",
    "                cats[-1][item] = 1.0\n",
    "            cat_sequences.append(cats)\n",
    "        return np.array(cat_sequences)\n",
    "\n",
    "def data_prep(random_seed):\n",
    "    random.seed(random_seed)\n",
    "    training_data_size = 20000\n",
    "    e2i_vocab_ortho = []\n",
    "\n",
    "    x_tr_ortho = []\n",
    "    y_tr = []\n",
    "\n",
    "    x_val_ortho = []\n",
    "    y_val = []\n",
    "\n",
    "    orig_file = open('data/post_clean.txt')\n",
    "\n",
    "    orig_data = orig_file.readlines()\n",
    "    orig_file.close()\n",
    "    orig_data = [line.strip('\\n') for line in orig_data]\n",
    "    random.shuffle(orig_data)\n",
    "    data_eng = [line.split(';')[0].lower() for line in orig_data]\n",
    "    data_syl = [line.split(';')[1].lower() for line in orig_data]\n",
    "    y_tr = []\n",
    "\n",
    "    for line in data_eng:\n",
    "        for c in line:\n",
    "            if c not in e2i_vocab_ortho:\n",
    "                e2i_vocab_ortho += [c]\n",
    "\n",
    "    e2i_ortho = dict((a,i) for i,a in enumerate(e2i_vocab_ortho, 1))\n",
    "\n",
    "    for line in data_eng[:training_data_size]:\n",
    "        converted = []\n",
    "        for c in line:\n",
    "            converted += [e2i_ortho[c]]\n",
    "        x_tr_ortho += [converted]\n",
    "\n",
    "    x_tr_ortho = pad_sequences(x_tr_ortho, maxlen=20, padding='post')\n",
    "\n",
    "    for line in data_syl[:training_data_size]:\n",
    "        y_tr += [convert_to_hot(line)]\n",
    "\n",
    "    y_tr = pad_sequences(y_tr, maxlen=20, padding='post')\n",
    "    \n",
    "    x_val_ortho = []\n",
    "    y_val = []\n",
    "\n",
    "    for line in data_eng[training_data_size:]:\n",
    "        converted = []\n",
    "        for c in line:\n",
    "            converted += [e2i_ortho[c]]\n",
    "        x_val_ortho += [converted]\n",
    "\n",
    "    x_val_ortho = pad_sequences(x_val_ortho, maxlen=20, padding='post')\n",
    "\n",
    "    for line in data_syl[training_data_size:]:\n",
    "        y_val += [convert_to_hot(line)]\n",
    "\n",
    "    y_val = pad_sequences(y_val, maxlen=20, padding='post')\n",
    "    return x_tr_ortho, y_tr, x_val_ortho, y_val, e2i_ortho\n",
    "\n",
    "def training_split(x_tr_ortho, y_tr):\n",
    "    y_tr = to_categorical(y_tr)\n",
    "    split_index = int(.8 * len(x_tr_ortho))\n",
    "\n",
    "    x_test_ortho = x_tr_ortho[split_index:]\n",
    "    y_test = y_tr[split_index:]\n",
    "\n",
    "    x_tr_ortho = x_tr_ortho[:split_index]\n",
    "    y_tr = y_tr[:split_index]\n",
    "    return x_tr_ortho, x_test_ortho, y_tr, y_test\n",
    "\n",
    "def train_model(sp, x_tr_ortho, y_tr, x_test_ortho, y_test, run_id):\n",
    "    sp.fit(x_tr_ortho, y_tr, x_test_ortho, y_test, ep=70, batch_size=128, save_filename=\"eval_runs/%i_single_pen_best_weights.h5\"%run_id, verbose=0)\n",
    "    \n",
    "def sp_attempts(sp, x_val_ortho):\n",
    "    attempts = []\n",
    "    for i in range(0, len(x_val_ortho)):\n",
    "        attempts += [sp.raw_syllabify(x_val_ortho[i])]\n",
    "        print(i, end='\\r')\n",
    "    attempts_stripped = []\n",
    "    for x in attempts:\n",
    "        attempts_stripped += [[i for i in x if i !=0]]\n",
    "    return attempts_stripped\n",
    "\n",
    "def back_to_eng(x_val_ortho, e2i_ortho):\n",
    "    converted_back_to_eng = []\n",
    "    for x in x_val_ortho:\n",
    "        real_word = \"\"\n",
    "        for i in x:\n",
    "                if i != 0:\n",
    "                    real_word += list(e2i_ortho.keys())[list(e2i_ortho.values()).index(i)]\n",
    "        converted_back_to_eng += [real_word]\n",
    "    return converted_back_to_eng\n",
    "\n",
    "def insert_and_rehot(sp, attempts, converted_back_to_eng):\n",
    "    eng_conv_attempts = []\n",
    "    for i in range(0, len(attempts)):\n",
    "        eng_conv_attempts += [sp.insert_syl(converted_back_to_eng[i], attempts[i])]\n",
    "    rehot_attempts = []\n",
    "    for word in eng_conv_attempts:\n",
    "        rehot_attempts += [convert_to_hot(word)]\n",
    "    return rehot_attempts\n",
    "\n",
    "def hyphenator_run(converted_back_to_eng):\n",
    "    liang_attempts = []\n",
    "    for word in converted_back_to_eng:\n",
    "        liang_attempts += ['-'.join(hyphenate_word(word))]\n",
    "\n",
    "    liang_attempts_hot_encoded = [convert_to_hot(word) for word in liang_attempts]\n",
    "    return liang_attempts_hot_encoded\n",
    "\n",
    "def pyph_run(converted_back_to_eng):\n",
    "    Pyphenator = pyphen.Pyphen(lang='en_US', left=1, right=1)\n",
    "    pyph_attempts = []\n",
    "    for word in converted_back_to_eng:\n",
    "        pyph_attempts += [Pyphenator.inserted(word)]\n",
    "    pyph_attempts_hot =  [convert_to_hot(word) for word in pyph_attempts]\n",
    "    return pyph_attempts_hot\n",
    "\n",
    "def inconsistency_grab(sp, attempts, reals, converted_back_to_eng, run_id, sp_hyph):\n",
    "    filename = 'final_evaluation/incorrect_syls/'+ sp_hyph + '_run_%i_incorrect_syls.txt'%run_id\n",
    "    sum_lev = 0\n",
    "    incorrect_counter = 0\n",
    "    incorrect_syl_count = 0\n",
    "    file = open(filename, 'w+', encoding='utf-8')\n",
    "    file.write('Attempt' + '\\t' + 'Real' + '\\n')\n",
    "    for i in range(0, len(attempts)):\n",
    "        if attempts[i] != reals[i]:\n",
    "            incorrect_counter += 1\n",
    "            a = sp.insert_syl(converted_back_to_eng[i], attempts[i])\n",
    "            r = sp.insert_syl(converted_back_to_eng[i], reals[i])\n",
    "            if len(a.split('-')) != len(r.split('-')):\n",
    "                incorrect_syl_count += 1\n",
    "            sum_lev += edit_distance(a,r,substitution_cost=1, transpositions=True)\n",
    "            file.write(a + '\\t' + r + '\\n')\n",
    "            \n",
    "    syllable_accuracy = ((len(attempts) - incorrect_syl_count)/len(attempts))\n",
    "    av_lev_dist = (sum_lev/incorrect_counter)\n",
    "    file.write(\"Words with errors: %i\"%incorrect_counter +'\\n')\n",
    "    file.write(\"Words with incorrect number of syllables: %i\"%incorrect_syl_count +'\\n')\n",
    "    file.write(\"Total evluated: %i\"%len(attempts) +'\\n')\n",
    "    file.write(\"Perfect accuracy: %.2f\"%((len(attempts) - incorrect_counter)/len(attempts)) +'\\n')\n",
    "    file.write(\"Number of syllables accuracy: %.2f\"%((len(attempts) - incorrect_syl_count)/len(attempts)) +'\\n')\n",
    "    file.write(\"Average Levenshtein Distance(across incorrect words): %.2f\"%(sum_lev/incorrect_counter) +'\\n')\n",
    "    file.close()\n",
    "    return syllable_accuracy, av_lev_dist\n",
    "\n",
    "def strip_y_val(y_val):\n",
    "    reals = []\n",
    "    for x in y_val:\n",
    "        reals += [[i for i in x if i !=0]]\n",
    "    return reals\n",
    "\n",
    "def store_run_elements(sp, back_to_eng, attempts, reals, run_id, model):\n",
    "    a = sp.insert_syl(back_to_eng[0],attempts[0])\n",
    "    r = sp.insert_syl(back_to_eng[0],reals[0])\n",
    "    curr_df = pd.DataFrame(columns=['given_word', 'real_syllabification', 'attempted_syllabification', 'lev_dist', 'true_syl_count', 'attempted_syl_count'])\n",
    "    for i in range(1, len(attempts)):\n",
    "        word_stats = {'given_word':back_to_eng[i], 'real_syllabification':r, 'attempted_syllabification':a, 'lev_dist':'null', 'true_syl_count':'null', 'attempted_syl_count':'null'}\n",
    "        word_stats['lev_dist'] = edit_distance(a,r,substitution_cost=1, transpositions=True)\n",
    "        word_stats['attempted_syl_count'] = len(a.split('-'))\n",
    "        word_stats['true_syl_count'] = len(r.split('-'))\n",
    "        curr_df = curr_df.append(word_stats,ignore_index=True)\n",
    "    curr_df.to_csv(\"final_evaluation/individual_elements/run_%i_%s_word_stats.csv\"%(run_id,model), sep=',', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45a57586-0e81-4d8b-a86d-101bf5948a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run: 1\n",
      "Begin sp training, run: 1\n",
      "Epoch 54: early stopping\n",
      "End sp training, run: 1\n",
      "Begin sp attempts, run: 1\n",
      "Completed sp attempts, run: 1\n",
      "run: 2\n",
      "Begin sp training, run: 2\n",
      "Epoch 65: early stopping\n",
      "End sp training, run: 2\n",
      "Begin sp attempts, run: 2\n",
      "Completed sp attempts, run: 2\n",
      "run: 3\n",
      "Begin sp training, run: 3\n",
      "Epoch 66: early stopping\n",
      "End sp training, run: 3\n",
      "Begin sp attempts, run: 3\n",
      "Completed sp attempts, run: 3\n",
      "run: 4\n",
      "Begin sp training, run: 4\n",
      "Epoch 64: early stopping\n",
      "End sp training, run: 4\n",
      "Begin sp attempts, run: 4\n",
      "Completed sp attempts, run: 4\n",
      "run: 5\n",
      "Begin sp training, run: 5\n",
      "Epoch 54: early stopping\n",
      "End sp training, run: 5\n",
      "Begin sp attempts, run: 5\n",
      "Completed sp attempts, run: 5\n",
      "run: 6\n",
      "Begin sp training, run: 6\n",
      "Epoch 63: early stopping\n",
      "End sp training, run: 6\n",
      "Begin sp attempts, run: 6\n",
      "Completed sp attempts, run: 6\n",
      "run: 7\n",
      "Begin sp training, run: 7\n",
      "Epoch 51: early stopping\n",
      "End sp training, run: 7\n",
      "Begin sp attempts, run: 7\n",
      "Completed sp attempts, run: 7\n",
      "run: 8\n",
      "Begin sp training, run: 8\n",
      "End sp training, run: 8\n",
      "Begin sp attempts, run: 8\n",
      "Completed sp attempts, run: 8\n",
      "run: 9\n",
      "Begin sp training, run: 9\n",
      "Epoch 61: early stopping\n",
      "End sp training, run: 9\n",
      "Begin sp attempts, run: 9\n",
      "Completed sp attempts, run: 9\n",
      "run: 10\n",
      "Begin sp training, run: 10\n",
      "Epoch 54: early stopping\n",
      "End sp training, run: 10\n",
      "Begin sp attempts, run: 10\n",
      "Completed sp attempts, run: 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_id</th>\n",
       "      <th>random_seed</th>\n",
       "      <th>val_sample_size</th>\n",
       "      <th>training_time_seconds</th>\n",
       "      <th>y_val_syl_prob</th>\n",
       "      <th>sp_f1_score</th>\n",
       "      <th>sp_precision</th>\n",
       "      <th>sp_recall</th>\n",
       "      <th>sp_brier_score</th>\n",
       "      <th>sp_syllable_accuracy</th>\n",
       "      <th>...</th>\n",
       "      <th>hyph_recall</th>\n",
       "      <th>hyph_brier_score</th>\n",
       "      <th>hyph_syllable_accuracy</th>\n",
       "      <th>hyph_average_lev</th>\n",
       "      <th>pyph_f1_score</th>\n",
       "      <th>pyph_precision</th>\n",
       "      <th>pyph_recall</th>\n",
       "      <th>pyph_brier_score</th>\n",
       "      <th>pyph_syllable_accuracy</th>\n",
       "      <th>pyph_average_lev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.682568e+09</td>\n",
       "      <td>6500.0</td>\n",
       "      <td>392.342035</td>\n",
       "      <td>0.208093</td>\n",
       "      <td>0.896362</td>\n",
       "      <td>0.899934</td>\n",
       "      <td>0.892817</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.898462</td>\n",
       "      <td>...</td>\n",
       "      <td>0.806848</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.732462</td>\n",
       "      <td>1.206074</td>\n",
       "      <td>0.859414</td>\n",
       "      <td>0.902961</td>\n",
       "      <td>0.819873</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.688462</td>\n",
       "      <td>1.235824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.682569e+09</td>\n",
       "      <td>6500.0</td>\n",
       "      <td>495.376108</td>\n",
       "      <td>0.208393</td>\n",
       "      <td>0.905397</td>\n",
       "      <td>0.906458</td>\n",
       "      <td>0.904339</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.899692</td>\n",
       "      <td>...</td>\n",
       "      <td>0.808958</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.733538</td>\n",
       "      <td>1.187839</td>\n",
       "      <td>0.858457</td>\n",
       "      <td>0.900596</td>\n",
       "      <td>0.820086</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.682000</td>\n",
       "      <td>1.213687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.682570e+09</td>\n",
       "      <td>6500.0</td>\n",
       "      <td>609.537641</td>\n",
       "      <td>0.206333</td>\n",
       "      <td>0.904579</td>\n",
       "      <td>0.903247</td>\n",
       "      <td>0.905914</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.899385</td>\n",
       "      <td>...</td>\n",
       "      <td>0.803638</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.729385</td>\n",
       "      <td>1.187332</td>\n",
       "      <td>0.856687</td>\n",
       "      <td>0.899885</td>\n",
       "      <td>0.817446</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.684000</td>\n",
       "      <td>1.217835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.682571e+09</td>\n",
       "      <td>6500.0</td>\n",
       "      <td>672.423171</td>\n",
       "      <td>0.208356</td>\n",
       "      <td>0.906900</td>\n",
       "      <td>0.906095</td>\n",
       "      <td>0.907705</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.904000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.805966</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.728000</td>\n",
       "      <td>1.197108</td>\n",
       "      <td>0.857941</td>\n",
       "      <td>0.901266</td>\n",
       "      <td>0.818590</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.681846</td>\n",
       "      <td>1.217660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>1.682573e+09</td>\n",
       "      <td>6500.0</td>\n",
       "      <td>982.439931</td>\n",
       "      <td>0.206777</td>\n",
       "      <td>0.902138</td>\n",
       "      <td>0.897237</td>\n",
       "      <td>0.907093</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.902308</td>\n",
       "      <td>...</td>\n",
       "      <td>0.804906</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.729231</td>\n",
       "      <td>1.173936</td>\n",
       "      <td>0.855189</td>\n",
       "      <td>0.896634</td>\n",
       "      <td>0.817407</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.681385</td>\n",
       "      <td>1.207047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.0</td>\n",
       "      <td>1.682574e+09</td>\n",
       "      <td>6500.0</td>\n",
       "      <td>1619.894697</td>\n",
       "      <td>0.208747</td>\n",
       "      <td>0.907235</td>\n",
       "      <td>0.906183</td>\n",
       "      <td>0.908288</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.903846</td>\n",
       "      <td>...</td>\n",
       "      <td>0.810816</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.733692</td>\n",
       "      <td>1.191908</td>\n",
       "      <td>0.862678</td>\n",
       "      <td>0.905940</td>\n",
       "      <td>0.823360</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.692000</td>\n",
       "      <td>1.219800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7.0</td>\n",
       "      <td>1.682577e+09</td>\n",
       "      <td>6500.0</td>\n",
       "      <td>1679.219550</td>\n",
       "      <td>0.208108</td>\n",
       "      <td>0.900715</td>\n",
       "      <td>0.889426</td>\n",
       "      <td>0.912295</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.894462</td>\n",
       "      <td>...</td>\n",
       "      <td>0.809911</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.733538</td>\n",
       "      <td>1.187093</td>\n",
       "      <td>0.859489</td>\n",
       "      <td>0.900369</td>\n",
       "      <td>0.822160</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.685538</td>\n",
       "      <td>1.214222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8.0</td>\n",
       "      <td>1.682579e+09</td>\n",
       "      <td>6500.0</td>\n",
       "      <td>2949.424448</td>\n",
       "      <td>0.207341</td>\n",
       "      <td>0.908118</td>\n",
       "      <td>0.902534</td>\n",
       "      <td>0.913772</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.894769</td>\n",
       "      <td>...</td>\n",
       "      <td>0.806947</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.729692</td>\n",
       "      <td>1.203235</td>\n",
       "      <td>0.855313</td>\n",
       "      <td>0.895189</td>\n",
       "      <td>0.818837</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.676000</td>\n",
       "      <td>1.224729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9.0</td>\n",
       "      <td>1.682583e+09</td>\n",
       "      <td>6500.0</td>\n",
       "      <td>3196.814972</td>\n",
       "      <td>0.206732</td>\n",
       "      <td>0.901961</td>\n",
       "      <td>0.894302</td>\n",
       "      <td>0.909752</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.893846</td>\n",
       "      <td>...</td>\n",
       "      <td>0.809686</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.732308</td>\n",
       "      <td>1.185225</td>\n",
       "      <td>0.858270</td>\n",
       "      <td>0.898503</td>\n",
       "      <td>0.821486</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.686769</td>\n",
       "      <td>1.215747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10.0</td>\n",
       "      <td>1.682587e+09</td>\n",
       "      <td>6500.0</td>\n",
       "      <td>3421.688818</td>\n",
       "      <td>0.207336</td>\n",
       "      <td>0.901400</td>\n",
       "      <td>0.903744</td>\n",
       "      <td>0.899069</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.893692</td>\n",
       "      <td>...</td>\n",
       "      <td>0.812153</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.736615</td>\n",
       "      <td>1.190980</td>\n",
       "      <td>0.862301</td>\n",
       "      <td>0.903878</td>\n",
       "      <td>0.824382</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.691385</td>\n",
       "      <td>1.223996</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   run_id   random_seed  val_sample_size  training_time_seconds  \\\n",
       "0     1.0  1.682568e+09           6500.0             392.342035   \n",
       "1     2.0  1.682569e+09           6500.0             495.376108   \n",
       "2     3.0  1.682570e+09           6500.0             609.537641   \n",
       "3     4.0  1.682571e+09           6500.0             672.423171   \n",
       "4     5.0  1.682573e+09           6500.0             982.439931   \n",
       "5     6.0  1.682574e+09           6500.0            1619.894697   \n",
       "6     7.0  1.682577e+09           6500.0            1679.219550   \n",
       "7     8.0  1.682579e+09           6500.0            2949.424448   \n",
       "8     9.0  1.682583e+09           6500.0            3196.814972   \n",
       "9    10.0  1.682587e+09           6500.0            3421.688818   \n",
       "\n",
       "   y_val_syl_prob  sp_f1_score  sp_precision  sp_recall  sp_brier_score  \\\n",
       "0        0.208093     0.896362      0.899934   0.892817             0.0   \n",
       "1        0.208393     0.905397      0.906458   0.904339             0.0   \n",
       "2        0.206333     0.904579      0.903247   0.905914             0.0   \n",
       "3        0.208356     0.906900      0.906095   0.907705             0.0   \n",
       "4        0.206777     0.902138      0.897237   0.907093             0.0   \n",
       "5        0.208747     0.907235      0.906183   0.908288             0.0   \n",
       "6        0.208108     0.900715      0.889426   0.912295             0.0   \n",
       "7        0.207341     0.908118      0.902534   0.913772             0.0   \n",
       "8        0.206732     0.901961      0.894302   0.909752             0.0   \n",
       "9        0.207336     0.901400      0.903744   0.899069             0.0   \n",
       "\n",
       "   sp_syllable_accuracy  ...  hyph_recall  hyph_brier_score  \\\n",
       "0              0.898462  ...     0.806848               0.0   \n",
       "1              0.899692  ...     0.808958               0.0   \n",
       "2              0.899385  ...     0.803638               0.0   \n",
       "3              0.904000  ...     0.805966               0.0   \n",
       "4              0.902308  ...     0.804906               0.0   \n",
       "5              0.903846  ...     0.810816               0.0   \n",
       "6              0.894462  ...     0.809911               0.0   \n",
       "7              0.894769  ...     0.806947               0.0   \n",
       "8              0.893846  ...     0.809686               0.0   \n",
       "9              0.893692  ...     0.812153               0.0   \n",
       "\n",
       "   hyph_syllable_accuracy  hyph_average_lev  pyph_f1_score  pyph_precision  \\\n",
       "0                0.732462          1.206074       0.859414        0.902961   \n",
       "1                0.733538          1.187839       0.858457        0.900596   \n",
       "2                0.729385          1.187332       0.856687        0.899885   \n",
       "3                0.728000          1.197108       0.857941        0.901266   \n",
       "4                0.729231          1.173936       0.855189        0.896634   \n",
       "5                0.733692          1.191908       0.862678        0.905940   \n",
       "6                0.733538          1.187093       0.859489        0.900369   \n",
       "7                0.729692          1.203235       0.855313        0.895189   \n",
       "8                0.732308          1.185225       0.858270        0.898503   \n",
       "9                0.736615          1.190980       0.862301        0.903878   \n",
       "\n",
       "   pyph_recall  pyph_brier_score  pyph_syllable_accuracy  pyph_average_lev  \n",
       "0     0.819873               0.0                0.688462          1.235824  \n",
       "1     0.820086               0.0                0.682000          1.213687  \n",
       "2     0.817446               0.0                0.684000          1.217835  \n",
       "3     0.818590               0.0                0.681846          1.217660  \n",
       "4     0.817407               0.0                0.681385          1.207047  \n",
       "5     0.823360               0.0                0.692000          1.219800  \n",
       "6     0.822160               0.0                0.685538          1.214222  \n",
       "7     0.818837               0.0                0.676000          1.224729  \n",
       "8     0.821486               0.0                0.686769          1.215747  \n",
       "9     0.824382               0.0                0.691385          1.223996  \n",
       "\n",
       "[10 rows x 23 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 3\n",
    "# full run\n",
    "\n",
    "# instantiating a dataframe for recording run data\n",
    "df = pd.DataFrame(columns=['run_id', 'random_seed','val_sample_size','training_time_seconds' ,'y_val_syl_prob', 'sp_f1_score', 'sp_precision', 'sp_recall','sp_brier_score','sp_syllable_accuracy','sp_average_lev', 'hyph_f1_score','hyph_precision', 'hyph_recall', 'hyph_brier_score','hyph_syllable_accuracy','hyph_average_lev','pyph_f1_score','pyph_precision', 'pyph_recall', 'pyph_brier_score','pyph_syllable_accuracy','pyph_average_lev'])\n",
    "sp = None\n",
    "\n",
    "for i in range(1,11):\n",
    "    print(\"run: %i\"%i)\n",
    "    random_seed = int(time.time())\n",
    "    run_stats = {'run_id':i, 'random_seed':random_seed,'val_sample_size':'null','training_time_seconds':'null', 'y_val_syl_prob':'null', 'sp_f1_score':'null', 'sp_precision':'null', 'sp_recall':'null','sp_brier_score':'null','sp_syllable_accuracy':'null','sp_average_lev':'null', 'hyph_f1_score':'null','hyph_precision':'null', 'hyph_recall':'null', 'hyph_brier_score':'null','hyph_syllable_accuracy':'null','hyph_average_lev':'null', 'pyph_f1_score':'null','pyph_precision':'null', 'pyph_recall':'null', 'pyph_brier_score':'null','pyph_syllable_accuracy':'null','pyph_average_lev':'null'}\n",
    "    \n",
    "    x_tr_ortho, y_tr, x_val_ortho, y_val, e2i_ortho = data_prep(random_seed=random_seed)\n",
    "    \n",
    "    run_stats['val_sample_size'] = len(x_val_ortho)\n",
    "    \n",
    "    x_tr_ortho, x_test_ortho, y_tr, y_test = training_split(x_tr_ortho=x_tr_ortho, y_tr=y_tr)\n",
    "    \n",
    "    del sp\n",
    "    sp = sp_syllabler(e2i_ortho= e2i_ortho, ortho_input_size=20,latent_dim=32,embed_dim=32 ,max_feat=36)\n",
    "    \n",
    "    start = time.time()\n",
    "    print(\"Begin sp training, run: %i\"%i)\n",
    "    train_model(sp, x_tr_ortho, y_tr, x_test_ortho, y_test, run_id=i)\n",
    "    print(\"End sp training, run: %i\"%i)\n",
    "    end = time.time()\n",
    "    run_stats['training_time_seconds'] = end - start\n",
    "    \n",
    "    print(\"Begin sp attempts, run: %i\"%i)\n",
    "    sp_attempts_array = sp_attempts(sp, x_val_ortho)\n",
    "    print(\"Completed sp attempts, run: %i\"%i)\n",
    "    \n",
    "    converted_back_to_eng = back_to_eng(x_val_ortho, e2i_ortho)\n",
    "    \n",
    "    sp_rehot_attempts = insert_and_rehot(sp, sp_attempts_array, converted_back_to_eng)\n",
    "    \n",
    "    liang_attempts_hot_encoded = hyphenator_run(converted_back_to_eng)\n",
    "    \n",
    "    syl_prob = get_probability(y_val)\n",
    "    \n",
    "    run_stats['y_val_syl_prob'] = syl_prob\n",
    "    \n",
    "    reals = strip_y_val(y_val)\n",
    "    \n",
    "    sp_total_checked, sp_correct_num_char, sp_true_pos, sp_true_neg, sp_false_pos, sp_false_neg, sp_precision, sp_recall, sp_f_one = calc_f1(sp_rehot_attempts, reals)\n",
    "    sp_brier = calc_brier(sp_rehot_attempts, syl_prob)\n",
    "    \n",
    "    run_stats['sp_f1_score'] = sp_f_one\n",
    "    run_stats['sp_precision'] = sp_precision\n",
    "    run_stats['sp_recall'] = sp_recall\n",
    "    run_stats['sp_brier_score'] = sp_brier\n",
    "    \n",
    "    hyph_total_checked, hyph_correct_num_char, hyph_true_pos, hyph_true_neg, hyph_false_pos, hyph_false_neg, hyph_precision, hyph_recall, hyph_f_one = calc_f1(liang_attempts_hot_encoded, reals)\n",
    "    hyph_brier = calc_brier(liang_attempts_hot_encoded, syl_prob)\n",
    "    \n",
    "    run_stats['hyph_f1_score'] = hyph_f_one\n",
    "    run_stats['hyph_precision'] = hyph_precision\n",
    "    run_stats['hyph_recall'] = hyph_recall\n",
    "    run_stats['hyph_brier_score'] = hyph_brier\n",
    "    \n",
    "    pyph_attempts_hot = pyph_run(converted_back_to_eng)\n",
    "    pyph_total_checked, pyph_correct_num_char, pyph_true_pos, pyph_true_neg, pyph_false_pos, pyph_false_neg, pyph_precision, pyph_recall, pyph_f_one = calc_f1(pyph_attempts_hot, reals)\n",
    "    pyph_brier = calc_brier(pyph_attempts_hot, syl_prob)\n",
    "    \n",
    "    run_stats['hyph_f1_score'] = hyph_f_one\n",
    "    run_stats['hyph_precision'] = hyph_precision\n",
    "    run_stats['hyph_recall'] = hyph_recall\n",
    "    run_stats['hyph_brier_score'] = hyph_brier\n",
    "    \n",
    "    run_stats['pyph_f1_score'] = pyph_f_one\n",
    "    run_stats['pyph_precision'] = pyph_precision\n",
    "    run_stats['pyph_recall'] = pyph_recall\n",
    "    run_stats['pyph_brier_score'] = pyph_brier\n",
    "    \n",
    "    sp_syllable_accuracy, sp_av_lev_dist = inconsistency_grab(sp, sp_rehot_attempts, reals, converted_back_to_eng, run_id=i, sp_hyph='sp')\n",
    "    hyph_syllable_accuracy, hyph_av_lev_dist = inconsistency_grab(sp, liang_attempts_hot_encoded, reals, converted_back_to_eng, run_id=i, sp_hyph='hyph')\n",
    "    pyph_syllable_accuracy, pyph_av_lev_dist = inconsistency_grab(sp, pyph_attempts_hot, reals, converted_back_to_eng, run_id=i, sp_hyph='pyph')\n",
    "    \n",
    "    \n",
    "    run_stats['sp_syllable_accuracy'] = sp_syllable_accuracy\n",
    "    run_stats['sp_average_lev'] = sp_av_lev_dist\n",
    "    \n",
    "    run_stats['hyph_syllable_accuracy'] = hyph_syllable_accuracy\n",
    "    run_stats['hyph_average_lev'] = hyph_av_lev_dist\n",
    "    \n",
    "    run_stats['pyph_syllable_accuracy'] = pyph_syllable_accuracy\n",
    "    run_stats['pyph_average_lev'] = pyph_av_lev_dist\n",
    "    \n",
    "    store_run_elements(sp,converted_back_to_eng, sp_rehot_attempts, reals, i, 'sp')\n",
    "    store_run_elements(sp,converted_back_to_eng, liang_attempts_hot_encoded, reals, i, 'hyph')\n",
    "    store_run_elements(sp,converted_back_to_eng, pyph_attempts_hot, reals, i, 'pyph')\n",
    "    \n",
    "    df = df.append(run_stats,ignore_index=True)\n",
    "    df.to_csv(\"final_evaluation/run_%i_liang_sp_comparison.csv\"%i, sep=',', index=False, encoding='utf-8')\n",
    "\n",
    "df.to_csv('final_evaluation/total_liang_sp_comparison.csv', sep=',', index=False, encoding='utf-8')\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6eaa139-36d3-4349-9e16-13b131991d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross comparing syllable accuracy with syllables\n",
    "\n",
    "syllables_results = []\n",
    "\n",
    "for random_seed in df['random_seed']:\n",
    "    true_num_syl = []\n",
    "    x_tr_ortho, y_tr, x_val_ortho, y_val, e2i_ortho = data_prep(random_seed=random_seed)\n",
    "    converted_back_to_eng = back_to_eng(x_val_ortho, e2i_ortho)\n",
    "    for x in y_val:\n",
    "        num_syls = 1\n",
    "        for c in x:\n",
    "            if c == 2:\n",
    "                num_syls += 1\n",
    "        true_num_syl += [num_syls]\n",
    "    syl_attempt = []\n",
    "    for word in converted_back_to_eng:\n",
    "        syl_attempt += [syllables.estimate(word)]\n",
    "    \n",
    "    correct_syls = 0\n",
    "    for i in range(0, len(syl_attempt)):\n",
    "        if syl_attempt[i] == true_num_syl[i]:\n",
    "            correct_syls += 1\n",
    "    syllables_results += [float(correct_syls)/float(len(syl_attempt))]\n",
    "    \n",
    "df['syllable_module_syl_count_accuracy'] = syllables_results\n",
    "display(df)\n",
    "df.to_csv('final_evaluation/total_liang_syllables_sp_comparison.csv', sep=',', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cd08b6-63ee-4bc1-adc7-f4b9d8706873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross comparing with pyphen\n",
    "Pyphenator = pyphen.Pyphen(lang='en_US', left=1, right=1)\n",
    "\n",
    "pyph_f1_arr = []\n",
    "pyph_precision_arr = []\n",
    "pyph_recall_arr = []\n",
    "pyph_av_lev_arr = []\n",
    "pyph_brier_arr = []\n",
    "pyph_syllable_accuracy_arr = []\n",
    "\n",
    "run_num = 0\n",
    "\n",
    "for random_seed in df['random_seed']:\n",
    "    run_num += 1\n",
    "    x_tr_ortho, y_tr, x_val_ortho, y_val, e2i_ortho = data_prep(random_seed=random_seed)\n",
    "    converted_back_to_eng = back_to_eng(x_val_ortho, e2i_ortho)\n",
    "    \n",
    "    pyph_attempts = []\n",
    "    for x in converted_back_to_eng:\n",
    "        pyph_attempts += [Pyphenator.inserted(x)]\n",
    "    pyph_attempts_hot = [convert_to_hot(x) for x in pyph_attempts]\n",
    "    \n",
    "    syl_prob = get_probability(y_val)\n",
    "    \n",
    "    reals = strip_y_val(y_val)\n",
    "    \n",
    "    pyph_total_checked, pyph_correct_num_char, pyph_true_pos, pyph_true_neg, pyph_false_pos, pyph_false_neg, pyph_precision, pyph_recall, pyph_f_one = calc_f1(pyph_attempts_hot, reals)\n",
    "    pyph_brier = calc_brier(pyph_attempts_hot, syl_prob)\n",
    "    pyph_syllable_accuracy, pyph_av_lev_dist = inconsistency_grab(sp, pyph_attempts_hot, reals, converted_back_to_eng, run_id=run_num, sp_hyph='pyph')\n",
    "    \n",
    "    pyph_f1_arr += [pyph_f_one]\n",
    "    pyph_precision_arr += [pyph_precision]\n",
    "    pyph_recall_arr += [pyph_recall]\n",
    "    pyph_av_lev_arr += [pyph_av_lev_dist]\n",
    "    pyph_brier_arr += [pyph_brier]\n",
    "    pyph_syllable_accuracy_arr += [pyph_syllable_accuracy]\n",
    "    \n",
    "df['pyph_f1_score'] = pyph_f1_arr\n",
    "df['pyph_precision'] = pyph_precision_arr\n",
    "df['pyph_recall'] = pyph_recall_arr\n",
    "df['pyph_brier_score'] = pyph_brier_arr\n",
    "df['pyph_syllable_accuracy'] = pyph_syllable_accuracy_arr\n",
    "df['pyph_average_lev'] = pyph_av_lev_arr\n",
    "\n",
    "display(df)\n",
    "df.to_csv('final_evaluation/total_liang_syllables_sp_pyph_comparison.csv', sep=',', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcdaf62-e414-4e77-a80d-ba5d60e1734a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Pyphenator = pyphen.Pyphen(lang='en_US', left=1, right=1)\n",
    "\n",
    "word = 'iterator'.encode()\n",
    "word = 'iterator'\n",
    "\n",
    "print(Pyphenator.inserted(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16be5927-ac54-4c51-b61d-4da354bf3444",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_hyph_f1_score = 0\n",
    "for x in df['hyph_f1_score']:\n",
    "    sum_hyph_f1_score += x\n",
    "print('average hyph f1:')\n",
    "print(sum_hyph_f1_score/10)\n",
    "\n",
    "sum_hyph_syl_acc = 0\n",
    "for x in df['hyph_syllable_accuracy']:\n",
    "    sum_hyph_syl_acc += x\n",
    "print('average hyph syl acc:')\n",
    "print(sum_hyph_syl_acc/10)\n",
    "\n",
    "sum_hyph_lev = 0\n",
    "for x in df['hyph_average_lev']:\n",
    "    sum_hyph_lev += x\n",
    "print('average hyph lev:')\n",
    "print(sum_hyph_lev/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd075f31-0815-45d8-8399-616cad341400",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_sp_f1_score = 0\n",
    "for x in df['sp_f1_score']:\n",
    "    sum_sp_f1_score += x\n",
    "print('average sp f1:')\n",
    "print(sum_sp_f1_score/10)\n",
    "\n",
    "sum_sp_syl_acc = 0\n",
    "for x in df['sp_syllable_accuracy']:\n",
    "    sum_sp_syl_acc += x\n",
    "print('average sp syl acc:')\n",
    "print(sum_sp_syl_acc/10)\n",
    "\n",
    "sum_sp_lev = 0\n",
    "for x in df['sp_average_lev']:\n",
    "    sum_sp_lev += x\n",
    "print('average sp lev:')\n",
    "print(sum_hyph_lev/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e1baa7-0115-4b3d-a3df-e6d241cf0d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('standard deviation sp_f1_score:')\n",
    "print(np.std(df['sp_f1_score']))\n",
    "print('standard deviation sp_syllable_accuracy:')\n",
    "print(np.std(df['sp_syllable_accuracy']))\n",
    "print('standard deviation sp_lev:')\n",
    "print(np.std(df['sp_average_lev']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae4d435-cf05-46c2-9c71-a34acf23fe05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "laptop_sketchbook",
   "language": "python",
   "name": "laptop_sketchbook"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
