{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b4425c2-da36-46cb-ad55-78f687d29943",
   "metadata": {},
   "source": [
    "# Overview of Notebook\n",
    "I don't believe scraping from the oxford dictionary is necessarily conforming to an ethical standard, so I will try and find a dataset to validate the myph datset other than the scraped oxford dictionary.\n",
    "\n",
    "I'm primarily looking for a human curated dataset.\n",
    "\n",
    "I'm going to the use the curated dataset from [DelphiForFun](http://www.delphiforfun.org/programs/Syllables.htm), by Gary D. Darby (God rest his soul). Darby used the mhyph dataset to generate a set of rules for syllabification. A limitation of his work is that his syllabification algorithm is not capable of syllabifying all English words, or pseudowords.\n",
    "\n",
    "I'm going to use my syllabification network, trained on his dataset, for a more robust system of syllabification, one that is ethically sourced this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f88c6923-c9c5-4760-8a9e-d4a7f1890b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "# imports\n",
    "import pickle\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n",
    "import re\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd83ba3c-4179-4845-9c99-3c1943dd93a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2\n",
    "# helper functions\n",
    "\n",
    "def convert_to_hot(syl_wor, split_token):\n",
    "    hot = []\n",
    "    i = 0\n",
    "    while  i < len(syl_word):\n",
    "        if i == len(syl_word) - 1:\n",
    "            hot += [1]\n",
    "            return hot\n",
    "        if syl_word[i+1] == split_token:\n",
    "            hot += [2]\n",
    "            i += 2\n",
    "        else:\n",
    "            hot += [1]\n",
    "            i += 1\n",
    "    return hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "276cd28f-c2fb-4381-9914-6350052568f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3\n",
    "# environment constants\n",
    "\n",
    "data_raw = 'data/Syllables.txt'\n",
    "train_size = 8000\n",
    "dev_size = 1000\n",
    "test_size = 1000\n",
    "\n",
    "output_directory = 'preprocessed'\n",
    "\n",
    "split_token = 'Â·'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ae836f7-61e2-430c-bc19-6c1962d7476b",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/Syllables.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 4\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# open the file, shuffle and sample\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_raw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m      5\u001b[0m     raw \u001b[38;5;241m=\u001b[39m [line\u001b[38;5;241m.\u001b[39mstrip(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m file\u001b[38;5;241m.\u001b[39mget_lines()]\n\u001b[0;32m      6\u001b[0m     random\u001b[38;5;241m.\u001b[39mshuffle(raw)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\syllabification-2024\\lib\\site-packages\\IPython\\core\\interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    284\u001b[0m     )\n\u001b[1;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/Syllables.txt'"
     ]
    }
   ],
   "source": [
    "# 4\n",
    "# open the file, shuffle and sample\n",
    "\n",
    "with open(data_raw, 'r', encoding='utf-8') as file:\n",
    "    raw = [line.strip('\\n').split('=') for line in file.get_lines()]\n",
    "    random.shuffle(raw)\n",
    "    train = raw[:train_size]\n",
    "    dev = raw[train_size: train_size + dev_size]\n",
    "    test = raw[train_size + dev_size: train_size + dev_size + test_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8225ac89-1937-4cbf-bece-9a507b090b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5\n",
    "# extracted train, dev, test, format into x,y with one hot\n",
    "xtr = [x for x,y in train]\n",
    "ytr = [convert_to_hot(y, split_token) for x,y in train]\n",
    "\n",
    "xdev = [x for x,y in dev]\n",
    "ydev = [convert_to_hot(y, split_token) for x,y in dev]\n",
    "\n",
    "xte = [x for x,y in test]\n",
    "yte = [convert_to_hot(y, split_token) for x,y in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5156bba9-c7b0-4da3-8fce-7b137a5045d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " 'mhyph.txt',\n",
       " 'ox',\n",
       " 'post_clean.txt',\n",
       " 'scraped.txt',\n",
       " 'scraped_clean_clean.txt']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297b363f-b18d-4e08-bd7e-0743647fec81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
